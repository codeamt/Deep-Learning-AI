<h1 align="center">DLAI-2: Improving Deep Neural Networks</h1>

<p align="center">
<img src="https://ucarecdn.com/04185dea-98f5-4727-86d3-8c4976e030db/" width="70%" height="60%">
</p>


<h1 align="center">About the Course</h1>
In the [second course]() of deeplearning.ai, Andrew Ng helps students migrate away from the "black box" approach to deep learning, and instead better understand optimization techniques and intuitions that give deep neural nets a performance boost. 

Some of the deep learning "tricks" explored include: how to initialize weights for learning, optimization alogrithms (e.g., mini-batch gradient descent, Adam and RMSprop) and how to check for their convergence, regularization (e.g., L2 and dropout), batch normalization and common ratios for splitting up data into training, dev, and test sets, and more. 

Up until this course, the specialization focuses mostly on the binary classifier as the algorithm of choice. In the final week of this course, Professor Ng introduces a new type of output layer -- the "Softmax" layer -- which sets the stage for vector encoding in later courses. 

Once all the fundamentals of building optimal deep NN have been covered and we have fine grain understanding of how all the moving parts work and how best to tune our hyperparameters, the programming assignments gets more applied with a brief intro on how to implement a simple computational graph with the popular DL framework, Tensorflow. 


## Lessons

- [x] Practical Aspects of Deep Learning
- [x] Optimizing Algorithms
- [x] Tuning Hyperparameters and Batch Normalization


<p align="center">
<img src="https://ucarecdn.com/b9c85edb-9a45-47b0-a466-dd177783745b/" width="60%" height="50%">
</p>



## Programming Assignments

- [x] [Initialization](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/1%20Practical%20Aspects%20of%20Deep%20Learning/1-PA/README.md)
- [x] [Regularization](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/1%20Practical%20Aspects%20of%20Deep%20Learning/2-PA/README.md)
- [x] [Gradient Checking](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/1%20Practical%20Aspects%20of%20Deep%20Learning/3-PA/README.md)
- [x] [Optimization](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/2%20Optimization%20Algorithms/1-PA/README.md)
- [x] [Tensorflow](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/3%20Tuning%20Hyperparameters%20and%20Batch%20Normalization/1-PA/README.md)



## Additional Material

<p align="center">
  <b>Hereos of Deep Learning Interview with Yoshua Bengio</b><br>
<img src="" width="400px" height="350px"><br>
  Here's where I'll give a brief synopsis of the interview and my takeaways.
</p>

<p align="center">
  <b>Heroes of Deep Learning Interview with Yuanqing Lin</b><br>
<img src="" width="400px" height="350px"><br>
  Here's where I'll give a brief synopsis of the interview and my takeaways.
</p>
