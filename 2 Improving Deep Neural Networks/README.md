<h1 align="center">DLAI-2: Improving Deep Neural Networks</h1>

<p align="center">
<img src="https://ucarecdn.com/04185dea-98f5-4727-86d3-8c4976e030db/" width="70%" height="60%">
</p>


<h1 align="center">About the Course</h1>
In the [second course](https://www.coursera.org/learn/deep-neural-network) of deeplearning.ai, Andrew Ng helps students migrate away from the "black box" approach to deep learning, and instead better understand optimization techniques and intuitions that give deep neural nets a performance boost. 

Some of the deep learning "tricks" explored include: 

- how to initialize weights for learning 
- optimization alogrithms (e.g., mini-batch gradient descent, Adam and RMSprop) and how to check for their convergence 
- regularization (e.g., L2 and dropout) 
- batch normalization 
- common ratio splits of data for training, dev, and test sets 
- more 

Up until this course, the specialization focuses mostly on the binary classifier as the DL algorithm of choice. In the final week of this course, Professor Ng introduces a new type of output layer - the "Softmax" layer - which sets the stage for vector encoding in later courses. 

Once all the best practices for tuning hyperparameters have been covered, the course gets into popular frameworks that handle some of the heavy lifting with built-in modular methods that we've been implementing from scratch in our programming assignments. This "learn-the-proof" teaching style helps to better understand all the moving parts of a DL framework, like Tensorflow, which we use in our final programming assignment to implement a simple computational graph (linked below). 


## Lessons

- [x] Practical Aspects of Deep Learning
- [x] Optimizing Algorithms
- [x] Tuning Hyperparameters and Batch Normalization


<p align="center">
<img src="https://ucarecdn.com/b9c85edb-9a45-47b0-a466-dd177783745b/" width="60%" height="50%">
</p>



## Programming Assignments

- [x] [Initialization](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/1%20Practical%20Aspects%20of%20Deep%20Learning/1-PA/README.md)
- [x] [Regularization](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/1%20Practical%20Aspects%20of%20Deep%20Learning/2-PA/README.md)
- [x] [Gradient Checking](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/1%20Practical%20Aspects%20of%20Deep%20Learning/3-PA/README.md)
- [x] [Optimization](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/2%20Optimization%20Algorithms/1-PA/README.md)
- [x] [Tensorflow](https://github.com/codeamt/Deep-Learning-AI/blob/master/2%20Improving%20Deep%20Neural%20Networks/Implementations/3%20Tuning%20Hyperparameters%20and%20Batch%20Normalization/1-PA/README.md)



## Additional Material

<p align="center">
  <b>Hereos of Deep Learning Interview with Yoshua Bengio</b><br>
<img src="" width="400px" height="350px"><br>
  Here's where I'll give a brief synopsis of the interview and my takeaways.
</p>

<p align="center">
  <b>Heroes of Deep Learning Interview with Yuanqing Lin</b><br>
<img src="" width="400px" height="350px"><br>
  Here's where I'll give a brief synopsis of the interview and my takeaways.
</p>
