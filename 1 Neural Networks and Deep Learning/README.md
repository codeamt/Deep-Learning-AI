<h1 align="center">DLAI-1: Neural Network and Deep Learning</h1>


<p align="center">
<img src="https://ucarecdn.com/cf7d2668-ef01-489f-a591-c8f6b4274c2d/" width="70%" height="60%">
</p>

<h1 align="center">About the Course</h1>

In the [first course](https://www.coursera.org/learn/neural-networks-deep-learning) of deeplearning.ai, Andrew Ng provides a thoughtful introduction to the foundational concepts of deep learning. 

Deep learning, a subsect of Machine Learning, denotes the regressive process of learning: 

- the appropriate co-efficients ("the weights") and bias values (together, "the parameters") for multi-dimensional inputs (meaning, featurized x values - say, a record of houses and respective features, like number of bedrooms, bathrooms, windows, etc., for those houses) 

- uses these learned parameters to architect a schematic formula (a model) that predicts outputs (y values, sometimes also multi-dimensional, but following the example, say the market price) for those inputs with a freakishly high level of accuracy. 

We start off basic in the first week, and unpack how logistic regression (weight generation), gradient descent (weight correction) and cost calculation (error analysis) work for a single training example (e.g., a single house-to-price mapping),  and move outward to apply these processes for some (n) number of training examples (e.g., ALL the houses, all in one loop), first in a single perceptron (one layer), 

<p align="center">
<img src="https://ucarecdn.com/0b85575b-6c0f-434d-8f9e-d01ca7f0c732/" width="60%" height="50%"
</p>
  
then, in a multi-layer perceptron, or a deep network.

<p align="center">
<img src="https://ucarecdn.com/70d6d8d2-699e-4869-8df3-5d9a8558a7c0/" width="60%" height="50%" >
</p>

The magic of deep learning happens in the inner parts of an architected model, using aforementioned layers of what we call "neurons" that activate and facilitate the learning process. The actual "learning" happens with the help of Calculus and derivatives, when we perform "back propogation" to systematically improve our model's learning.

<p align="center">
  <img src="https://ucarecdn.com/2acdb538-145d-4abe-8a65-e722fd8feb5c/" width="60%" height="50%" >
</p>

Professor Ng goes beyond providing the theory, and demonstrates how to implement this process in actual Python code, including methods for vectorizing some (n) number of inputs with some (m) number of features to decrease the time complexity of operations and speed up the weight learning process.

## Lessons

- [x] Neural Network Basics
- [x] Shallow Neural Networks
- [x] Deep Neural Networks


<p align="center">
<img src="https://ucarecdn.com/8b064603-8fc5-471b-80d8-4a621d2c665d/" width="60%" height="50%">
</p>


## Programming Assignments

- [x] [Logistic Regression with a Neural Network mindset](https://github.com/codeamt/Deep-Learning-AI/tree/master/1%20Neural%20Networks%20and%20Deep%20Learning/Implementations/2%20Neural%20Networks%20Basics/2-PA)
- [x] [Planar data classification with a hidden layer](https://github.com/codeamt/Deep-Learning-AI/blob/master/1%20Neural%20Networks%20and%20Deep%20Learning/Implementations/3%20Shallow%20Neural%20Networks/1-PA/README.md)
- [x] [Building your deep neural network: step by step](https://github.com/codeamt/Deep-Learning-AI/blob/master/1%20Neural%20Networks%20and%20Deep%20Learning/Implementations/4%20Deep%20Neural%20Networks/1-PA/README.md)
- [x] [Deep Neural Network Application](https://github.com/codeamt/Deep-Learning-AI/blob/master/1%20Neural%20Networks%20and%20Deep%20Learning/Implementations/4%20Deep%20Neural%20Networks/2-PA/README.md)


## Additional Material

Heroes of Deep Learning Interviews:


<p align="center">
 **Geoffrey Hinton**
  <br>
<img src="" width="400px" height="350px">
</p>


<p align="center">
  **Ian Goodfellow**
  <br>
<img src="" width="400px" height="350px">
</p>


<p align="center">
  **Pieter Abbeel**
  <br>
<img src="" width="400px" height="350px">
</p>
