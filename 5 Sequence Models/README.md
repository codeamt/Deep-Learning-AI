<h1 align="center">DLAI-5: Sequence Models</h1>

<p align="center">
<img src="https://ucarecdn.com/e9905bea-68f5-49a6-a839-8816e42bc1bb/" width="70%" height="60%">
</p>

<h1 align="center">About the Course</h1>e

In the fifth and final course of the specialization, we explore Recurrent Neural Networks (RNNs) and training on sequences/time series data. 

Why do we need a new type of network to handle sequence data? Mostly, because input sequences might not be the same length and doesn't necessarily share features further down the input. 



## Lessons
- [x] Recurrent Neural Networks
- [x] Natural Language Processing & Word Embeddings
- [x] Sequence models & Attention mechanism

<p align="center">
<img src="https://ucarecdn.com/48d0c7e4-5cc3-4502-bf2b-76b2f2e47cbd/" width="60%" height="50%">
</p>

## Python Implementations

- [x] [Building a recurrent neural network: step by step](https://github.com/codeamt/Deep-Learning-AI/blob/master/5%20Sequence%20Models/Implementations/1%20RNNs/1-PA/README.md)
- [x] [Dinosaur Island - Character-Level Language Modeling](https://github.com/codeamt/Deep-Learning-AI/blob/master/5%20Sequence%20Models/Implementations/1%20RNNs/2-PA/README.md)
- [x] [Jazz improvisation with LSTM](https://github.com/codeamt/Deep-Learning-AI/blob/master/5%20Sequence%20Models/Implementations/1%20RNNs/3-PA/README.md)
- [x] [Operations on word vectors - Debiasing](https://github.com/codeamt/Deep-Learning-AI/blob/master/5%20Sequence%20Models/Implementations/2%20NLP%20and%20Word%20Embeddings/1-PA/README.md)
- [x] [Emojify](https://github.com/codeamt/Deep-Learning-AI/blob/master/5%20Sequence%20Models/Implementations/2%20NLP%20and%20Word%20Embeddings/2-PA/README.md)
- [x] [Neural Machine Translation with Attention](https://github.com/codeamt/Deep-Learning-AI/blob/master/5%20Sequence%20Models/Implementations/3%20Sequence%20Models%20and%20Attention%20Mechanism%20/1-PA/README.md)
- [x] [Trigger word detection](https://github.com/codeamt/Deep-Learning-AI/blob/master/5%20Sequence%20Models/Implementations/3%20Sequence%20Models%20and%20Attention%20Mechanism%20/2-PA/README.md)


## Additional Material

**Reviewed Research Papers:**

[1]() Cho et al., 2014. *On the properties of neural machine translation: Encoder-decoderapproaches*
[2]() Chung et al., 2014. *Empirical Evaluation of Gated Recurrent Neural Network*
[3]() Hochreiter and Schmidchuber 1997. *Long short-term memory.* 
[4]() van der Marteen and Hinton., 2008. *Visualizing data using t-SNE.* 
[5]() Mikolov et al., 2013. *Linguistic regularities in continuous space with representations*
[6]() Bengio et al., 2003. *A neural probabilistic language model.*
[7]() Mikolov et al., 2013. *Efficient representation of word representation in vector space* 
[8]() Mikolov et al., 2013. *Distributed representations of words and phrases and their compositionality* 
[9]() Pennington et al., 2014. *GloVe: Global vectors for word representations.* 
[10]() Bolukbasi et al., 2016. *Man is to computer programmer as woman is to homemaker? Debiasing word embeddings?* 
[11]() Sutskever et al., 2014. *Sequence to sequence learning with neural networks*
[12]() Cho et al., 2014. *Learning phrase representations using RNN encoder/decoder for statistical machine translation*
[13]() Mao et al., 2014. *Deep captioning with multimodal recurrent neural network*
[14]() Vinyals et al., 2014. *Show and tell: Neural image captioning generator*
[15]() Karpathy and Fei Fei, 2015. *Deep visual-semantic alignments for generating image descriptions* 
[16]() Papineni et al., 2002. *A method for automatic evaluation of machine translation*
[17]() Bahdanau et al., 2014. *Neural machine translation by jointly learning to align and translate*
[18]() Xu et al., 2015. *Show attention and tell: neural image caption generation with visual attention* 
[19]() Graves et al., 2006. *Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks*

